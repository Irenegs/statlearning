---
title: "Exercises without data set"
output: html_notebook
---

**Índice**   
1. [Chapter 3 - Exercise 11](#id1)  
2. [Chapter 3 - Exercise 12](#id2)  
3. [Chapter 3 - Exercise 13](#id3)  
4. [Chapter 3 - Exercise 14](#id4)
5. [Chapter 4 - Exercise 15](#id5)
6. [Chapter 5 - Exercise 8](#id6)


<h1>Chapter 3 - Exercise 11</h1><a name="id1"></a>

In this problem we will investigate the t-statistic for the null hypothesis H 0 : β = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.

```{r}
set.seed(1)
x = rnorm(100)
y =2*x + rnorm(100)
```

(a) Perform a simple linear regression of y onto x , without an intercept. Report the coefficient estimate B, the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis H 0 : β = 0. Comment on these results. (You can perform regression without an intercept using the command lm(y∼x+0).)

```{r}
A=lm(y~x+0)
summary(A)
```

<span style='color:blue'>We reject the null hypothesis because p-value is low.</span>

(b) Now perform a simple linear regression of x onto y without an intercept, and report the coefficient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis H 0 : β = 0. Comment on these results.

```{r}
B=lm(x~y+0)
summary(B)
```

<span style='color:blue'>Same t-value, we reject the null hypothesis.</span>


(c) What is the relationship between the results obtained in (a) and (b)?

<span style='color:blue'>Same t-value, R² and F-statistic. A simple linear relationship is, so to say, simmetric: if y is significant to x, x is also significant to y. The coefficient and its interpretation varies: in (a) is the increase of y per unit of x and in (b) is the increase of x per unit of y.</span>

(d) For the regression of Y onto X without an intercept, the t-statistic for H 0 : β = 0 takes the form ^β/SE( ^β), where ^β is given by (3.38), and where 

SE(^β)=sqrt(sum((y-x*β)**2)/(n-1)* sum(x *x)).

(These formulas are slightly different from those given in Sections 3.1.1 and 3.1.2, since here we are performing regression without an intercept.) Show algebraically, and confirm numerically in R , that the t-statistic can be written as

(sqrt(length(x)-1))*sum(x*y)/sqrt(sum(x*x)*sum(y*y)-sum(x*y)**2)

<span style="color:blue">We show it only numerically. Recall that t-statistic is</span>

```{r}
Bsum <- summary(B)
Bsum$coefficients[,3]
```
<span style="color:blue">Now, we compute the t-statistic with the formula given in the subject.</span>
```{r}
(sqrt(length(x)-1))*sum(x*y)/sqrt(sum(x*x)*sum(y*y)-sum(x*y)**2)
```

(e) Using the results from (d), argue that the t-statistic for the regression of y onto x is the same as the t-statistic for the regression of x onto y .

<span style='color:blue'>Done: the formula is symmetric from x to y. Numerically is done in (a) and (b).</span>

(f) In R, show that when regression is performed with an intercept, the t-statistic for H 0 : β 1 = 0 is the same for the regression of y onto x as it is for the regression of x onto y .

```{r}
summary(lm(y~x))
```
```{r}
summary(lm(x~y))
```

<h1>Chapter 3 - Exercise 12</h1><a name="id2"></a>

This problem involves simple linear regression without an intercept.
(a) Recall that the coefficient estimate β̂ for the linear regression of Y onto X without an intercept is given by (3.38). Under what circumstance is the coefficient estimate for the regression of X onto Y the same as the coefficient estimate for the regression of Y onto X?

<span style='color:blue'>When suma(x^2) = suma(y^2)</span>


(b) Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is different from the coefficient estimate for the regression of Y onto X.

```{r}
x=c(1,100)
y=c(2,101)
summary(lm(x~y+0))
summary(lm(y~x+0))
```

(c) Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is the same as the coefficient estimate for the regression of Y onto X.

```{r}
x=c(1,100)
y=c(100,1)
summary(lm(x~y+0))
summary(lm(y~x+0))
```

<h1>Chapter 3 - Exercise 13</h1><a name="id3"></a>

In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.
(a) Using the rnorm() function, create a vector, x, containing 100 observations drawn from a N(0,1) distribution. This represents a feature, X.

```{r}
x=rnorm(100)
```

(b) Using the rnorm() function, create a vector, eps , containing 100 observations drawn from a N (0, 0.25) distribution i.e. a normal distribution with mean zero and variance 0.25.

```{r}
eps=rnorm(100, sd=0.25)
```

(c) Using x and eps, generate a vector y according to the model Y = −1 + 0.5X + e. (3.39) What is the length of the vector y? What are the values of β 0 and β 1 in this linear model?

```{r}
y=-1+0.5*x+eps
```

len(y)=100
B0=-1
B1=0.5

(d) Create a scatterplot displaying the relationship between x and y. Comment on what you observe.

```{r}
plot(x,y)
```


<span style='color:blue'>Positive quasi linear relationship. No leverage or special points.</span>

(e) Fit a least squares linear model to predict y using x . Comment on the model obtained. How do ^β_0 and ^β_1 compare to β 0 and β 1 ?

```{r}
E=lm(y~x)
summary(E)
```

<span style='color:blue'>The estimates for the coefficients are close to the real values Err(^β_0)=0,01211, Err(^β_1)=0,02655. Both estimations are greater than the real values. t-statistics, p-values and R² are OK.</span>

(f) Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend.

```{r}
plot(x,y)
abline(E, col='red')
legend(-3,-2.5,legend='LM(y~x)',lty = c(1, 2), col = c(2, 3), lwd = 2)
```


(g) Now fit a polynomial regression model that predicts y using x and x 2 . Is there evidence that the quadratic term improves the model fit? Explain your answer.

```{r}
F=lm(y~x+I(x^2))
summary(F)
```

<span style='color:blue'>The p-value for x² is high. There for there is no quadratic term.</span>

(h) Repeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. The model (3.39) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term eps in (b). Describe your results.

```{r}
eps2=rnorm(100, sd=0.1)
y2=-1+0.5*x+eps2
G=lm(y2~x)
summary(G)
```
```{r}
plot(x,y2)
abline(G, col='red')
abline(a=-1,b=0.5,col='green')
legend(x='topleft',legend=c('LM(y2~x)','y=-1+0.5x'),lty = c(1, 2), col = c(2, 3), lwd = 2)
```

<span style='color:blue'>The points are nearer to the abline, so the line makes better predictions.</span>

(i) Repeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data. The model (3.39) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term eps in (b). Describe your results.

```{r}
eps3=rnorm(100, sd=0.5)
y3=-1+0.5*x+eps3
H=lm(y3~x)
summary(H)
```
```{r}
plot(x,y3)
abline(H, col='red')
abline(a=-1,b=0.5,col='green')
legend(x='topleft',legend=c('LM(y3~x)','y=-1+0.5x'),lty = c(1, 2), col = c(2, 3), lwd = 2)

```

<span style='color:blue'>The points are further to the abline, so the line makes worse predictions.</span>

(j) What are the confidence intervals for β_0 and β_1 based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.

```{r}
print('Original data set')
confint(F)
print(confint(F)[1,2]-confint(F)[1,1])
print(confint(F)[2,2]-confint(F)[2,1])
print('Less noisy data set')
confint(G)
print(confint(G)[1,2]-confint(G)[1,1])
print(confint(G)[2,2]-confint(G)[2,1])
print('Noisier data set')
confint(H)
print(confint(H)[1,2]-confint(H)[1,1])
print(confint(H)[2,2]-confint(H)[2,1])
```

<span style='color:blue'>The noisier the data set, the wider the confident intervals.</span>

<h1>Chapter 3 - Exercise 14</h1><a name="id4"></a>

This problem focuses on the collinearity problem.
(a) Perform the following commands in R :

```{r}
set.seed(1)
x1 = runif(100)
x2 =0.5*x1 + rnorm(100)/10
y =2+2*x1 +0.3*x2 + rnorm(100)
```
The last line corresponds to creating a linear model in which y is a function of x1 and x2 . Write out the form of the linear model. What are the regression coefficients?

<span style='color:blue'>B0=2, B1=2, B2=0.3  
y = 2 + 2*x1 + 0.3*x2</span>

(b) What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.
```{r}
cor(x1,x2)
```

```{r}
plot(x,y)
```

(c) Using this data, fit a least squares regression to predict y using x1 and x2 . Describe the results obtained. What are β̂ 0 , β̂ 1 , and β̂ 2 ? How do these relate to the true β 0 , β 1 , and β 2 ? Can you reject the null hypothesis H 0 : β 1 = 0? How about the null hypothesis H 0 : β 2 = 0?

```{r}
I=lm(y~x1+x2)
summary(I)
```
<span style='color:blue'>B0=2.1305 for 2  
B1=1.4396 for 2  
B2=1.0097 for 0.3  
We can reject null hyphothesis H0: B1=0  (p-value < 0,05), but no B2=0, since p-value for B2 is 0.37 and t-value is close to 0.</span>

(d) Now fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis H 0 : β 1 = 0?

```{r}
J=lm(y ~ x1)
summary(J)
```

<span style='color:blue'>We can reject the null hyphotesis (low p-value for B1).</span>

(e) Now fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis H 0 : β 1 = 0?

```{r}
K=lm(y~x2)
summary(K)
```

<span style='color:blue'>We can reject the null hyphotesis (low p-value for B2).</span>

(f) Do the results obtained in (c)–(e) contradict each other? Explain your answer.

<span style='color:blue'>No, because there is an interaction between x1 and x2.</span>


(g) Now suppose we obtain one additional observation, which was unfortunately mismeasured.

```{r}
x1 = c ( x1 , 0.1)
x2 = c ( x2 , 0.8)
y = c (y ,6)
```
Re-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.
```{r}
L=lm(y~x1+x2)
summary(L)
```

```{r}
par(mfrow = c(2,2))
plot(L)
```

<span style='color:blue'>101-leverage  
Changes p-values for B1 and B2. x2 is more significant.</span>

```{r}
M=lm(y~x1)
summary(M)
```

```{r}
par(mfrow = c(2,2))
plot(M)
```

<span style='color:blue'>B0, B1, R² change  
101- outlier, but 82 and 55 also.</span>
```{r}
N=lm(y~x2)
summary(N)
```

```{r}
par(mfrow = c(2,2))
plot(N)
```

<span style='color:blue'>101 - leverage</span>

<h1>Chapter 4 - Exercise 15</h1><a name="id5"></a>

15. This problem involves writing functions.
(a) Write a function, Power() , that prints out the result of raising 2 to the 3rd power. In other words, your function should compute 2^3 and print out the results.
Hint: Recall that x^a raises x to the power a . Use the print() function to output the result.

```{r}
Power <- function(){
  print(2^3)
}
Power()
```


(b) Create a new function, Power2() , that allows you to pass any two numbers, x and a , and prints out the value of x^a . You can do this by beginning your function with the line> Power2 <- function (x , a ) {}
You should be able to call your function by entering, for instance, > Power2 (3 , 8)
on the command line. This should output the value of 3^8 , namely, 6561.

```{r}
Power2 <- function(x,a){print(x^a)}
Power2(3,8)
```


(c) Using the Power2() function that you just wrote, compute 10^3 , 8^17 , and 131^3 .

```{r}
Power2(10,3)
Power2(8,17)
Power2(131,3)
```


(d) Now create a new function, Power3() , that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called result within your function, then you can simply return() this return() result, using the following line: return ( result )The line above should be the last line in your function, before the } symbol.

```{r}
Power3 <- function(x,a){return(x^a)}
print(Power3(2,3))
```



(e) Now using the Power3() function, create a plot of f (x) = x^2 . The x-axis should display a range of integers from 1 to 10, and the y-axis should display x^2 . Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using log = "x" , log = "y" , or log = "xy" as arguments to the plot() function.

```{r}
x<-c(1:10)
y<-Power3(x,2)
plot(x,log="y")
```


(f) Create a function, PlotPower() , that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call> PlotPower (1:10 , 3) then a plot should be created with an x-axis taking on values 1, 2, . . . , 10, and a y-axis taking on values 1^3 ,2^3 ,..., 10^3 .

```{r}
PlotPower <- function(x,a){
  y<-Power3(x,a)
  plot(x,y)
}

PlotPower(x,2)
```

<h1>Chapter 5 - Exercise 8</h1><a name='id6'></a>

We will now perform cross-validation on a simulated data set.
(a) Generate a simulated data set as follows:

```{r}
set.seed (1)
x <- rnorm (100)
y <- x - 2 * x ^2 + rnorm (100)
```

In this data set, what is n and what is p? Write out the model used to generate the data in equation form.

p=2 and n=100. y=x+x^2+error

(b) Create a scatterplot of X against Y . Comment on what you find.

```{r}
plot(x,y)
```

<span style="color:blue">It seems a quadratic function.</span>

(c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:
i. Y = β 0 + β 1 X + ε
ii. Y = β 0 + β 1 X + β 2 X 2 + ε
iii. Y = β 0 + β 1 X + β 2 X 2 + β 3 X 3 + ε
iv. Y = β 0 + β 1 X + β 2 X 2 + β 3 X 3 + β 4 X 4 + ε.
Note you may find it helpful to use the data.frame() function to create a single data set containing both X and Y.

```{r}
#We create the data frame with x and y
DFxy <-data.frame(x,y)
```
```{r}
library(boot)
```

```{r}
set.seed(1)
cv.error <- c(0,4)
for (i in 1:4){
  LMi<-glm(y~poly(x,i), data=DFxy)#glm without family is just the lm() function
  cv.error[i]<- cv.glm(DFxy, LMi)$delta[1]#delta[1] is the std CV estimate, [2] is the bias-corrected version
}
cv.error
```

<span style="color:blue">The better results are obtained with the quadratic model.</span>

(d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?

```{r}
set.seed(5)
cv.error2 <- c(0,4)
for (i in 1:4){
  LMi<-glm(y~poly(x,i), data=DFxy)
  cv.error2[i]<- cv.glm(DFxy, LMi)$delta[1]
}
cv.error2
```

<span style="color:blue">Yes, because we use in LOOCV we use the entire DF leaving one out each time, but there is no random sample.</span>

(e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.

<span style="color:blue">The quadratic one. Yes, because the relation between x and y is almost quadratic.</span>

(f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?


```{r}
for (i in 1:4){
  LMi<-glm(y~poly(x,i), data=DFxy)
  print(i)
  print(summary(LMi))}
```
<span style="color:blue">We see that the best model is the quadratic one. We can not reject the null hypothesis for B^3 and B^4 since p-value is not small. In the quadratic model, B1 has lower p-value than in the linear model.</span>